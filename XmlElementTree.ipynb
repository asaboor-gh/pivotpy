{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp vr_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<style>a{text-decoration: none !important;color:lightkblue;font-weight:bold;}\n",
       "                a:focus,a:active,a:hover{color:hotpink !important;}</style>\n",
       "> [&nbsp;`▶` Index&nbsp;](https://massgh.github.io/pivotpy/)  \n",
       "> [&nbsp;`▶` XmlElementTree●&nbsp;](https://massgh.github.io/pivotpy/XmlElementTree)  \n",
       "> [&nbsp;`▶` StaticPlots&nbsp;](https://massgh.github.io/pivotpy/StaticPlots)  \n",
       "> [&nbsp;`▶` InteractivePlots&nbsp;](https://massgh.github.io/pivotpy/InteractivePlots)  \n",
       "> [&nbsp;`▶` Utilities&nbsp;](https://massgh.github.io/pivotpy/Utilities)  \n",
       "> [&nbsp;`▶` StructureIO&nbsp;](https://massgh.github.io/pivotpy/StructureIO)  \n",
       "> [&nbsp;`▶` Widgets&nbsp;](https://massgh.github.io/pivotpy/Widgets)  \n",
       "> [&nbsp;`▶` MainAPI&nbsp;](https://massgh.github.io/pivotpy/MainAPI)  \n",
       "> [&nbsp;`▶` SpinProjectedSurfaces&nbsp;](https://massgh.github.io/pivotpy/SpinProjectedSurfaces)  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "from pivotpy.utils import nav_links \n",
    "nav_links(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xml Parser\n",
    "> This parser contains functions to extract data from vasprun.xml. All functions in xml parser can work without arguments if working directory contains `vasprun.xml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Almost every object in this module returns a `Dict2Data` object with attributes accessible via dot notation. This object can by transformed to a dictionary by `to_dict()` method on the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "import os\n",
    "from itertools import islice, chain, product\n",
    "from collections import namedtuple\n",
    "from contextlib import suppress\n",
    "\n",
    "import numpy as np\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import textwrap\n",
    "import xml.etree.ElementTree as ET\n",
    "# Inside packages import to work both with package and jupyter notebook.\n",
    "try:\n",
    "    from pivotpy import utils as gu, serializer as serializer\n",
    "    from pivotpy.sio import read_ticks\n",
    "except:\n",
    "    import pivotpy.utils as gu\n",
    "    import pivotpy.serializer as serializer\n",
    "    import pivotpy.sio.read_ticks as read_ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "# To run notebook smoothly, not for module export\n",
    "from nbdev.showdoc import show_doc\n",
    "from pivotpy.vr_parser import load_export, islice2array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dict2tuple(name,d):\n",
    "    \"\"\"Converts a dictionary (nested as well) to namedtuple, accessible via index and dot notation as well as by unpacking.\n",
    "    - **Parameters**\n",
    "        - name: Name of the tuple.\n",
    "        - d   : Dictionary, nested works as well.\n",
    "    \"\"\"\n",
    "    return namedtuple(name,d.keys())(\n",
    "           *(dict2tuple(k.upper(),v) if isinstance(v,dict) else v for k,v in d.items())\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"Dict2Data\" class=\"doc_header\"><code>class</code> <code>Dict2Data</code><a href=\"https://github.com/massgh/pivotpy/tree/master/pivotpy/serializer.py#L19\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>Dict2Data</code>(**`d`**)\n",
       "\n",
       "- Returns a Data object with dictionary keys as attributes of Data accessible by dot notation or by key. Once an attribute is created, it can not be changed from outside.\n",
       "- **Parmeters**\n",
       "    - dict : Python dictionary (nested as well) containing any python data types.\n",
       "- **Methods**\n",
       "    - to_dict  : Converts a Data object to dictionary if it could be made a dictionary, otherwise throws relevant error.\n",
       "    - to_json  : Converts to json str or save to file if `outfil` given. Accepts `indent` as parameter.\n",
       "    - to_pickle: Converts to bytes str or save to file if `outfile` given.\n",
       "    - to_tuple : Converts to a named tuple.\n",
       "- **Example**\n",
       "    > x = Dict2Data({'A':1,'B':{'C':2}})\n",
       "    > x\n",
       "    > Data(\n",
       "    >     A = 1\n",
       "    >     B = Data(\n",
       "    >         C = 2\n",
       "    >         )\n",
       "    >     )\n",
       "    > x.B.to_dict()\n",
       "    > {'C': 2}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Dict2Data.to_dict\" class=\"doc_header\"><code>Dict2Data.to_dict</code><a href=\"https://github.com/massgh/pivotpy/tree/master/pivotpy/serializer.py#L52\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Dict2Data.to_dict</code>()\n",
       "\n",
       "Converts a `Dict2Data` object (root or nested level) to a dictionary.\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Dict2Data.to_json\" class=\"doc_header\"><code>Dict2Data.to_json</code><a href=\"https://github.com/massgh/pivotpy/tree/master/pivotpy/serializer.py#L62\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Dict2Data.to_json</code>(**`outfile`**=*`None`*, **`indent`**=*`1`*)\n",
       "\n",
       "Dumps a `Dict2Data` object (root or nested level) to json.\n",
       "- **Parameters**\n",
       "    - outfile : Default is None and returns string. If given, writes to file.\n",
       "    - indent  : Json indent. Default is 1."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Dict2Data.to_pickle\" class=\"doc_header\"><code>Dict2Data.to_pickle</code><a href=\"https://github.com/massgh/pivotpy/tree/master/pivotpy/serializer.py#L70\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Dict2Data.to_pickle</code>(**`outfile`**=*`None`*)\n",
       "\n",
       "Dumps a `Dict2Data` object (root or nested level) to pickle.\n",
       "- **Parameters**\n",
       "    - outfile : Default is None and returns string. If given, writes to file."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Dict2Data.to_tuple\" class=\"doc_header\"><code>Dict2Data.to_tuple</code><a href=\"https://github.com/massgh/pivotpy/tree/master/pivotpy/serializer.py#L77\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Dict2Data.to_tuple</code>()\n",
       "\n",
       "Creates a namedtuple."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(serializer.Dict2Data)\n",
    "show_doc(serializer.Dict2Data.to_dict)\n",
    "show_doc(serializer.Dict2Data.to_json)\n",
    "show_doc(serializer.Dict2Data.to_pickle)\n",
    "show_doc(serializer.Dict2Data.to_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict:  {'A': 1, 'B': 2}\n",
      "JSON:  {\n",
      " \"A\": 1,\n",
      " \"B\": 2\n",
      "}\n",
      "Pickle:  b'\\x80\\x03}q\\x00(X\\x01\\x00\\x00\\x00Aq\\x01K\\x01X\\x01\\x00\\x00\\x00Bq\\x02K\\x02u.'\n",
      "Tuple:  Data(A=1, B=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = serializer.Dict2Data({'A':1,'B':2})\n",
    "print('Dict: ',x.to_dict())\n",
    "print('JSON: ',x.to_json())\n",
    "print('Pickle: ',x.to_pickle())\n",
    "print('Tuple: ',x.to_tuple())\n",
    "x['A']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_asxml(path = None):\n",
    "    \"\"\"\n",
    "    - Reads a big vasprun.xml file into memory once and then apply commands. If current folder contains `vasprun.xml` file, it automatically picks it.\n",
    "\n",
    "    - **Parameters**\n",
    "        - path : Path/To/vasprun.xml\n",
    "\n",
    "    - **Returns**\n",
    "        - xml_data : Xml object to use in other functions\n",
    "    \"\"\"\n",
    "    path = path or './vasprun.xml'\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(\"File: '{}'' does not exist!\".format(path))\n",
    "        \n",
    "    elif 'vasprun.xml' not in path:\n",
    "        raise Exception(\"File name should end with 'vasprun.xml'\")\n",
    "        \n",
    "    fsize = gu.get_file_size(path)\n",
    "    value = float(fsize.split()[0])\n",
    "    print_str = \"\"\"\n",
    "    Memory Consumption Warning!\n",
    "    ---------------------------\n",
    "    File: {} is large ({}). It may consume a lot of memory (generally 3 times the file size).\n",
    "        An alternative way is to parse vasprun.xml is by using `Vasp2Visual` module in Powershell by command `pivotpy.load_export('path/to/vasprun.xml'), which runs underlying powershell functions to load data whith efficient memory managment. It works on Windows/Linux/MacOS if you have powershell core and Vasp2Visual installed on it.\n",
    "    \"\"\".format(path,fsize)\n",
    "    if 'MB' in fsize and value > 200:\n",
    "        print(gu.color.y(textwrap.dedent(print_str)))\n",
    "    elif 'GB' in fsize and value > 1:\n",
    "        print(gu.color.y(textwrap.dedent(print_str)))\n",
    "    \n",
    "    nt = namedtuple('VasprunXML',['path','root']) # Safe and full of info\n",
    "    return nt(os.path.abspath(path), ET.parse(path).getroot()) # THis is xml_data for other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def xml2dict(xmlnode_or_filepath):\n",
    "    \"\"\"Convert xml node or xml file content to dictionary. All output text is in string format, so further processing is required to convert into data types/split etc.\n",
    "    - The only paramenter `xmlnode_or_filepath` is either a path to an xml file or an `xml.etree.ElementTree.Element` object.\n",
    "    - Each node has `tag,text,attr,nodes` attributes. Every text element can be accessed via\n",
    "    `xml2dict()['nodes'][index]['nodes'][index]...` tree which makes it simple.\n",
    "    \"\"\"\n",
    "    if isinstance(xmlnode_or_filepath,str):\n",
    "        node = read_asxml(xmlnode_or_filepath)\n",
    "    else:\n",
    "        node = xmlnode_or_filepath\n",
    "\n",
    "    text = node.text.strip() if node.text else ''\n",
    "    nodes = [xml2dict(child) for child in list(node)]\n",
    "    return {'tag': node.tag,'text': text, 'attr':node.attrib, 'nodes': nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def exclude_kpts(xml_data):\n",
    "    \"\"\"\n",
    "    - Returns number of kpoints to exclude used from IBZKPT.\n",
    "    - **Parameters**\n",
    "        - xml_data : From `read_asxml` function\n",
    "    - **Returns**\n",
    "        - int      : Number of kpoints to exclude.\n",
    "    \"\"\"\n",
    "    for kpts in xml_data.root.iter('varray'):\n",
    "        if(kpts.attrib=={'name': 'weights'}):\n",
    "            weights=[float(arr.text.strip()) for arr in kpts.iter('v')]\n",
    "    exclude=[]\n",
    "    [exclude.append(item) for item in weights if item!=weights[-1]];\n",
    "    skipk=len(exclude) #that much to skip\n",
    "    return skipk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ispin(xml_data):\n",
    "    \"\"\"\n",
    "    - Returns value of ISPIN.\n",
    "    - **Parameters**\n",
    "        - xml_data : From `read_asxml` function\n",
    "    - **Returns**\n",
    "        - int      : Value of ISPIN.\n",
    "    \"\"\"\n",
    "    for item in xml_data.root.iter('i'):\n",
    "        if(item.attrib=={'type': 'int', 'name': 'ISPIN'}):\n",
    "            return int(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_kpoints_info(other_path = './vasprun.xml'):\n",
    "    \"Read KPOINTS file header and Cartesian/Reciprocal information based on other_path like vasprun.xml.\"\n",
    "    base_dir = os.path.split(os.path.abspath(other_path))[0]\n",
    "    path = os.path.join(base_dir,'KPOINTS')\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"File {path!r} not found. KPOINTS file should be in same directory as of {other_path!r}.\")\n",
    "    line = islice2array(path,start=2,nlines=1,exclude=None,raw=True).strip()\n",
    "    cart = True if line[0] in 'cCkK' else False \n",
    "    header = islice2array(path,start=0,nlines=1,exclude=None,raw=True).strip()\n",
    "    return serializer.Dict2Data({'cartesian':cart,'header':header})\n",
    "    \n",
    "\n",
    "def get_summary(xml_data):\n",
    "    \"\"\"\n",
    "    - Returns overview of system parameters.\n",
    "    - **Parameters**\n",
    "        - xml_data : From `read_asxml` function\n",
    "    - **Returns**\n",
    "        - Data     : pivotpy.Dict2Data with attibutes accessible via dot notation.\n",
    "    \"\"\"\n",
    "    for i_car in xml_data.root.iter('incar'):\n",
    "        incar={car.attrib['name']:car.text.strip() for car in i_car}\n",
    "    n_ions=[int(atom.text) for atom in xml_data.root.iter('atoms')][0]\n",
    "    type_ions=[int(atom_types.text) for atom_types in xml_data.root.iter('types')][0]\n",
    "    elem=[info[0].text.strip() for info in xml_data.root.iter('rc')]\n",
    "    elem_name=[]; #collect IONS names\n",
    "    [elem_name.append(item) for item in elem[:-type_ions] if item not in elem_name]\n",
    "    elem_index=[0]; #start index\n",
    "    [elem_index.append((int(entry)+elem_index[-1])) for entry in elem[-type_ions:]];\n",
    "    ISPIN=get_ispin(xml_data=xml_data)\n",
    "    NELECT = int([i.text.strip().split('.')[0] for i in xml_data.root.iter('i') if i.attrib['name']=='NELECT'][0])\n",
    "    # Fields\n",
    "    try:\n",
    "        for pro in xml_data.root.iter('partial'):\n",
    "            dos_fields=[field.text.strip() for field in pro.iter('field')]\n",
    "            dos_fields = [field for field in dos_fields if 'energy' not in field]\n",
    "    except:\n",
    "        dos_fields = []\n",
    "    for i in xml_data.root.iter('i'): #efermi for condition required.\n",
    "        if(i.attrib=={'name': 'efermi'}):\n",
    "            efermi=float(i.text)\n",
    "    #Writing information to a dictionary\n",
    "    info_dic={'SYSTEM':incar['SYSTEM'],'NION':n_ions,'NELECT':NELECT,'TypeION':type_ions,\n",
    "              'ElemName':elem_name,'ElemIndex':elem_index,'E_Fermi': efermi,'ISPIN':ISPIN,\n",
    "              'fields':dos_fields,'incar':incar,'kpts_info':get_kpoints_info(other_path = xml_data.path)}\n",
    "    return serializer.Dict2Data(info_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(SYSTEM='unknown system', NION=3, NELECT=14, TypeION=2, ElemName=['Mg', 'B'], ElemIndex=[0, 1, 3], E_Fermi=5.09505239, ISPIN=2, fields=['s', 'py', 'pz', 'px', 'dxy', 'dyz', 'dz2', 'dxz', 'dx2'], incar=INCAR(SYSTEM='unknown system', PREC='accurate', ALGO='Fast', ISPIN='2', ICHARG='1', NELM='100', IBRION='2', EDIFF='0.00000100', NSW='99', ISIF='3', ENCUT='520.00000000', MAGMOM='0.60000000      0.60000000      0.60000000', LREAL='Auto', ISMEAR='0', SIGMA='0.05000000', LWAVE='F', LORBIT='F'), kpts_info=KPTS_INFO(cartesian=False, header=\"Automatically generated using PivotPy with HSK-INDS = [0, 30, 60,-1], LABELS = ['Γ','M','K', 'Γ'], SEG-INDS = []\"))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pivotpy.vr_parser as vp\n",
    "xml_data = read_asxml(path= '../vasprun.xml')\n",
    "get_summary(xml_data=xml_data).to_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VasprunXML(path='E:\\\\Research\\\\vasprun.xml', root=<Element 'modeling' at 0x0000025EC459B3B8>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def join_ksegments(kpath,kseg_inds=[]):\n",
    "    \"\"\"Joins a broken kpath's next segment to previous. `kseg_inds` should be list of first index of next segment\"\"\"\n",
    "    path_list = np.array(kpath)\n",
    "    if kseg_inds:\n",
    "        for ind in kseg_inds:\n",
    "            path_list[ind:] -= path_list[ind] - path_list[ind-1]\n",
    "    return list(path_list)\n",
    "\n",
    "def get_kpts(xml_data, skipk = 0,kseg_inds=[]):\n",
    "    r\"\"\"Returns kpoints and calculated kpath.\n",
    "\n",
    "    **Parameters**\n",
    "    - xml_data: From `read_asxml` function.\n",
    "    - skipk : (int) Number of initil kpoints to skip.\n",
    "    - kseg_inds : (list) List of indices of kpoints where path is broken.\n",
    "\n",
    "    **Returns**\n",
    "    - Data : pivotpy.Dict2Data with attibutes `kpath` and `kpoints`.\n",
    "    \"\"\"\n",
    "    for kpts in xml_data.root.iter('varray'):\n",
    "        if(kpts.attrib=={'name': 'kpointlist'}):\n",
    "            kpoints=[[float(item) for item in arr.text.split()] for arr in kpts.iter('v')]\n",
    "    kpoints=np.array(kpoints[skipk:])\n",
    "    #KPath solved.\n",
    "    kpath=[0];pts=kpoints\n",
    "    [kpath.append(np.round(np.sqrt(np.sum((pt1-pt2)**2))+kpath[-1],6)) for pt1,pt2 in zip(pts[:-1],pts[1:])]\n",
    "    # If broken path, then join points.\n",
    "    kpath = join_ksegments(kpath,kseg_inds)\n",
    "    return serializer.Dict2Data({'NKPTS':len(kpoints),'kpoints':kpoints,'kpath':kpath})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(\n",
       "    NKPTS = 30\n",
       "    kpoints = <ndarray:shape=(30, 3)>\n",
       "    kpath = <list:len=30>\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_kpts(xml_data=xml_data,skipk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_tdos(xml_data,spin_set=1,elim=[]):\n",
    "    \"\"\"\n",
    "    - Returns total dos for a spin_set (default 1) and energy limit. If spin-polarized calculations, gives SpinUp and SpinDown keys as well.\n",
    "    - **Parameters**\n",
    "        - xml_data : From `read_asxml` function\n",
    "        - spin_set : int, default is 1 to pick up total spin.\n",
    "        - elim     : List [min,max] of energy, default empty.\n",
    "    - **Returns**\n",
    "        - Data     : pivotpy.Dict2Data with attibutes E_Fermi, ISPIN,tdos.\n",
    "    \"\"\"\n",
    "    tdos=[]; #assign for safely exit if wrong spin set entered.\n",
    "    ISPIN = get_ispin(xml_data=xml_data)\n",
    "    for neighbor in xml_data.root.iter('dos'):\n",
    "        for item in neighbor[1].iter('set'):\n",
    "            if(ISPIN==1 and spin_set==1):\n",
    "                if(item.attrib=={'comment': 'spin 1'}):\n",
    "                    tdos=np.array([[float(entry) for entry in arr.text.split()] for arr in item])\n",
    "            if(ISPIN==2 and spin_set==1):\n",
    "                if(item.attrib=={'comment': 'spin 1'}):\n",
    "                    tdos_1=np.array([[float(entry) for entry in arr.text.split()] for arr in item])\n",
    "                if(item.attrib=={'comment': 'spin 2'}):\n",
    "                    tdos_2=np.array([[float(entry) for entry in arr.text.split()] for arr in item])\n",
    "                    tdos = {'SpinUp':tdos_1,'SpinDown':tdos_2}\n",
    "            if(spin_set!=1): #can get any\n",
    "                if(item.attrib=={'comment': 'spin {}'.format(spin_set)}):\n",
    "                    tdos=np.array([[float(entry) for entry in arr.text.split()] for arr in item])\n",
    "    for i in xml_data.root.iter('i'): #efermi for condition required.\n",
    "        if(i.attrib=={'name': 'efermi'}):\n",
    "            efermi=float(i.text)\n",
    "    dos_dic= {'E_Fermi':efermi,'ISPIN':ISPIN,'tdos':tdos}\n",
    "    #Filtering in energy range.\n",
    "    if elim: #check if elim not empty\n",
    "        if(ISPIN==1 and spin_set==1):\n",
    "            up_ind=np.max(np.where(tdos[:,0]-efermi<=np.max(elim)))+1\n",
    "            lo_ind=np.min(np.where(tdos[:,0]-efermi>=np.min(elim)))\n",
    "            tdos=tdos[lo_ind:up_ind,:]\n",
    "        if(ISPIN==2 and spin_set==1):\n",
    "            up_ind=np.max(np.where(tdos['SpinUp'][:,0]-efermi<=np.max(elim)))+1\n",
    "            lo_ind=np.min(np.where(tdos['SpinUp'][:,0]-efermi>=np.min(elim)))\n",
    "            tdos = {'SpinUp':tdos_1[lo_ind:up_ind,:],'SpinDown':tdos_2[lo_ind:up_ind,:]}\n",
    "        if(spin_set!=1):\n",
    "            up_ind=np.max(np.where(tdos[:,0]-efermi<=np.max(elim)))+1\n",
    "            lo_ind=np.min(np.where(tdos[:,0]-efermi>=np.min(elim)))\n",
    "            tdos=tdos[lo_ind:up_ind,:]\n",
    "        dos_dic= {'E_Fermi':efermi,'ISPIN':ISPIN,'grid_range':range(lo_ind,up_ind),'tdos':tdos}\n",
    "    return serializer.Dict2Data(dos_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(\n",
       "    E_Fermi = 5.09505239\n",
       "    ISPIN = 2\n",
       "    tdos = Data(\n",
       "        SpinUp = <ndarray:shape=(301, 3)>\n",
       "        SpinDown = <ndarray:shape=(301, 3)>\n",
       "    )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tdos(xml_data=xml_data,spin_set=1,elim=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_evals(xml_data,skipk=None,elim=[]):\n",
    "    \"\"\"\n",
    "    - Returns eigenvalues as numpy array. If spin-polarized calculations, gives SpinUp and SpinDown keys as well.\n",
    "    - **Parameters**\n",
    "        - xml_data : From `read_asxml` function\n",
    "        - skipk    : Number of initil kpoints to skip.\n",
    "        - elim     : List [min,max] of energy, default empty.\n",
    "    - **Returns**\n",
    "        - Data     : pivotpy.Dict2Data with attibutes evals and related parameters.\n",
    "    \"\"\"\n",
    "    evals=[]; #assign for safely exit if wrong spin set entered.\n",
    "    ISPIN=get_ispin(xml_data=xml_data)\n",
    "    if skipk!=None:\n",
    "        skipk=skipk\n",
    "    else:\n",
    "        skipk=exclude_kpts(xml_data=xml_data) #that much to skip by default\n",
    "    for neighbor in xml_data.root.iter('eigenvalues'):\n",
    "        for item in neighbor[0].iter('set'):\n",
    "            if(ISPIN==1):\n",
    "                if(item.attrib=={'comment': 'spin 1'}):\n",
    "                    evals=np.array([[float(th.text.split()[0]) for th in thing] for thing in item])[skipk:]\n",
    "                    NBANDS=len(evals[0])\n",
    "            if(ISPIN==2):\n",
    "                if(item.attrib=={'comment': 'spin 1'}):\n",
    "                    eval_1=np.array([[float(th.text.split()[0]) for th in thing] for thing in item])[skipk:]\n",
    "                if(item.attrib=={'comment': 'spin 2'}):\n",
    "                    eval_2=np.array([[float(th.text.split()[0]) for th in thing] for thing in item])[skipk:]\n",
    "                    evals={'SpinUp':eval_1,'SpinDown':eval_2}\n",
    "                    NBANDS=len(eval_1[0])\n",
    "\n",
    "    for i in xml_data.root.iter('i'): #efermi for condition required.\n",
    "        if(i.attrib=={'name': 'efermi'}):\n",
    "            efermi=float(i.text)\n",
    "    evals_dic={'E_Fermi':efermi,'ISPIN':ISPIN,'NBANDS':NBANDS,'evals':evals,'indices': range(NBANDS)}\n",
    "    if elim: #check if elim not empty\n",
    "        if(ISPIN==1):\n",
    "            up_ind=np.max(np.where(evals[:,:]-efermi<=np.max(elim))[1])+1\n",
    "            lo_ind=np.min(np.where(evals[:,:]-efermi>=np.min(elim))[1])\n",
    "            evals=evals[:,lo_ind:up_ind]\n",
    "        if(ISPIN==2):\n",
    "            up_ind=np.max(np.where(eval_1[:,:]-efermi<=np.max(elim))[1])+1\n",
    "            lo_ind=np.min(np.where(eval_1[:,:]-efermi>=np.min(elim))[1])\n",
    "            evals={'SpinUp':eval_1[:,lo_ind:up_ind],'SpinDown':eval_2[:,lo_ind:up_ind]}\n",
    "        NBANDS = int(up_ind - lo_ind) #update Bands\n",
    "        evals_dic['NBANDS'] = NBANDS\n",
    "        evals_dic['indices'] = range(lo_ind,up_ind)\n",
    "        evals_dic['evals'] = evals\n",
    "    return serializer.Dict2Data(evals_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(\n",
       "    E_Fermi = 5.09505239\n",
       "    ISPIN = 2\n",
       "    NBANDS = 5\n",
       "    evals = Data(\n",
       "        SpinUp = <ndarray:shape=(30, 5)>\n",
       "        SpinDown = <ndarray:shape=(30, 5)>\n",
       "    )\n",
       "    indices = range(4, 9)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_evals(xml_data=xml_data,skipk=10,elim=[-5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_bands_pro_set(xml_data,\n",
    "                      spin_set=1,\n",
    "                      skipk=0,\n",
    "                      bands_range=None,\n",
    "                      set_path=None):\n",
    "    \"\"\"\n",
    "    - Returns bands projection of a spin_set(default 1). If spin-polarized calculations, gives SpinUp and SpinDown keys as well.\n",
    "    - **Parameters**\n",
    "        - xml_data    : From `read_asxml` function\n",
    "        - skipk       : Number of initil kpoints to skip (Default 0).\n",
    "        - spin_set    : Spin set to get, default is 1.\n",
    "        - bands_range : If elim used in `get_evals`,that will return bands_range to use here. Note that range(0,2) will give 2 bands 0,1 but tuple (0,2) will give 3 bands 0,1,2.\n",
    "        - set_path     : path/to/_set[1,2,3,4].txt, works if `split_vasprun` is used before.\n",
    "    - **Returns**\n",
    "        - Data     : pivotpy.Dict2Data with attibutes of bands projections and related parameters.\n",
    "    \"\"\"\n",
    "    if bands_range != None:\n",
    "        check_list = list(bands_range)\n",
    "        if check_list==[]:\n",
    "            raise ValueError(\"No bands prjections found in given energy range.\")\n",
    "    # Try to read _set.txt first. instance check is important.\n",
    "    if isinstance(set_path,str) and os.path.isfile(set_path):\n",
    "        _header = islice2array(set_path,nlines=1,raw=True,exclude=None)\n",
    "        _shape = [int(v) for v in _header.split('=')[1].strip().split(',')]\n",
    "        NKPTS, NBANDS, NIONS, NORBS = _shape\n",
    "        if NORBS == 3:\n",
    "            fields = ['s','p','d']\n",
    "        elif NORBS == 9:\n",
    "            fields = ['s','py','pz','px','dxy','dyz','dz2','dxz','x2-y2']\n",
    "        else:\n",
    "            fields = [str(i) for i in range(NORBS)] #s,p,d in indices.\n",
    "        COUNT = NIONS*NBANDS*(NKPTS-skipk)*NORBS\n",
    "        start = NBANDS*NIONS*skipk\n",
    "        nlines = None # Read till end.\n",
    "        if bands_range:\n",
    "            _b_r = list(bands_range)\n",
    "            # First line is comment but it is taken out by exclude in islice2array.\n",
    "            start = [[NIONS*NBANDS*k + NIONS*b for b in _b_r] for k in range(skipk,NKPTS)]\n",
    "            start = [s for ss in start for s in ss] #flatten\n",
    "            nlines = NIONS # 1 band has nions\n",
    "            NBANDS = _b_r[-1]-_b_r[0]+1 # upadte after start\n",
    "\n",
    "        NKPTS = NKPTS-skipk # Update after start, and bands_range.\n",
    "        COUNT = NIONS*NBANDS*NKPTS*NORBS\n",
    "        data = islice2array(set_path,start=start,nlines=nlines,count=COUNT)\n",
    "        data = data.reshape((NKPTS,NBANDS,NIONS,NORBS)).transpose([2,0,1,3])\n",
    "        return serializer.Dict2Data({'labels':fields,'pros':data})\n",
    "\n",
    "    #Collect Projection fields\n",
    "    fields=[];\n",
    "    for pro in xml_data.root.iter('projected'):\n",
    "        for arr in pro.iter('field'):\n",
    "            if('eig' not in arr.text and 'occ' not in arr.text):\n",
    "                fields.append(arr.text.strip())\n",
    "    NORBS = len(fields)\n",
    "    #Get NIONS for reshaping data\n",
    "    NIONS=[int(atom.text) for atom in xml_data.root.iter('atoms')][0]\n",
    "\n",
    "    for spin in xml_data.root.iter('set'):\n",
    "        if spin.attrib=={'comment': 'spin{}'.format(spin_set)}:\n",
    "            k_sets = [kp for kp in spin.iter('set') if 'kpoint' in kp.attrib['comment']]\n",
    "    k_sets = k_sets[skipk:]\n",
    "    NKPTS = len(k_sets)\n",
    "    band_sets = []\n",
    "    for k_s in k_sets:\n",
    "        b_set = [b for b in k_s.iter('set') if 'band' in b.attrib['comment']]\n",
    "        if bands_range == None:\n",
    "            band_sets.extend(b_set)\n",
    "        else:\n",
    "            b_r = list(bands_range)\n",
    "            band_sets.extend(b_set[b_r[0]:b_r[-1]+1])\n",
    "    NBANDS = int(len(band_sets)/len(k_sets))\n",
    "    try:\n",
    "        # Error prone solution but 5 times fater than list comprehension.\n",
    "        bands_pro = (float(t) for band in band_sets for l in band.iter('r') for t in l.text.split())\n",
    "        COUNT = NKPTS*NBANDS*NORBS*NIONS # Must be counted for performance.\n",
    "        data = np.fromiter(bands_pro,dtype=float,count=COUNT)\n",
    "    except:\n",
    "        # Alternate slow solution\n",
    "        print(\"Error using `np.fromiter`.\\nFalling back to (slow) list comprehension...\",end=' ')\n",
    "        bands_pro = (l.text for band in band_sets for l in band.iter('r'))\n",
    "        bands_pro = [[float(t) for t in text.split()] for text in bands_pro]\n",
    "        data = np.array(bands_pro)\n",
    "        del bands_pro # Release memory\n",
    "        print(\"Done.\")\n",
    "\n",
    "    data = data.reshape((NKPTS,NBANDS,NIONS,NORBS)).transpose((2,0,1,3))\n",
    "    return serializer.Dict2Data({'labels':fields,'pros':data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(\n",
       "    labels = ['s', 'py', 'pz', 'px', 'dxy', 'dyz', 'dz2', 'dxz', 'dx2']\n",
       "    pros = <ndarray:shape=(3, 40, 1, 9)>\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bands_pro_set(xml_data,skipk=0,spin_set=1,bands_range=range(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dos_pro_set(xml_data,spin_set=1,dos_range=None):\n",
    "    \"\"\"\n",
    "    - Returns dos projection of a spin_set(default 1) as numpy array. If spin-polarized calculations, gives SpinUp and SpinDown keys as well.\n",
    "    - **Parameters**\n",
    "        - xml_data    : From `read_asxml` function\n",
    "        - spin_set    : Spin set to get, default 1.\n",
    "        - dos_range   : If elim used in `get_tdos`,that will return dos_range to use here..\n",
    "    - **Returns**\n",
    "        - Data     : pivotpy.Dict2Data with attibutes of dos projections and related parameters.\n",
    "    \"\"\"\n",
    "    if dos_range != None:\n",
    "        check_list = list(dos_range)\n",
    "        if check_list == []:\n",
    "            raise ValueError(\"No DOS prjections found in given energy range.\")\n",
    "    \n",
    "    n_ions=get_summary(xml_data=xml_data).NION\n",
    "    for pro in xml_data.root.iter('partial'):\n",
    "        dos_fields=[field.text.strip()for field in pro.iter('field')]\n",
    "        #Collecting projections.\n",
    "        dos_pro=[]; set_pro=[]; #set_pro=[] in case spin set does not exists\n",
    "        for ion in range(n_ions):\n",
    "            for node in pro.iter('set'):\n",
    "                if(node.attrib=={'comment': 'ion {}'.format(ion+1)}):\n",
    "                    for spin in node.iter('set'):\n",
    "                        if(spin.attrib=={'comment': 'spin {}'.format(spin_set)}):\n",
    "                            set_pro=[[float(entry) for entry in r.text.split()] for r in spin.iter('r')]\n",
    "            dos_pro.append(set_pro)\n",
    "    if dos_range==None: #full grid computed.\n",
    "        dos_pro=np.array(dos_pro) #shape(NION,e_grid,pro_fields)\n",
    "    else:\n",
    "        dos_range=list(dos_range)\n",
    "        min_ind=dos_range[0]\n",
    "        max_ind=dos_range[-1]+1\n",
    "        dos_pro=np.array(dos_pro)[:,min_ind:max_ind,:]\n",
    "    final_data=np.array(dos_pro) #shape(NION,e_grid,pro_fields)\n",
    "    return serializer.Dict2Data({'labels':dos_fields,'pros':final_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _is_poscar_cartesian(other_path = './vaprun.xml'):\n",
    "    base_dir = os.path.split(os.path.abspath(other_path))[0]\n",
    "    _path = os.path.join(base_dir, 'POSCAR')\n",
    "    if not os.path.isfile(_path):\n",
    "        raise FileNotFoundError(f\"File {_path!r} not found. POSCAR file is required from same directory as of {other_path!r}.\")\n",
    "    lines = islice2array(_path,start=7,nlines=2,exclude=None,raw=True).splitlines()\n",
    "    lines = [l.strip() for l in lines] # remove whitespace or tabs\n",
    "    cart =  True if ((lines[0][0] in 'cCkK') or (lines[1][0] in 'cCkK')) else False\n",
    "    return cart\n",
    "\n",
    "def get_structure(xml_data):\n",
    "    \"\"\"\n",
    "    - Returns structure's volume,basis,positions and rec-basis.\n",
    "    - **Parameters**\n",
    "        - xml_data : From `read_asxml` function.\n",
    "    - **Returns**\n",
    "        - Data     : pivotpy.Dict2Data with attibutes volume,basis,positions rec_basis and labels.\n",
    "    \"\"\"\n",
    "    SYSTEM = [i.text for i in xml_data.root.iter('i') if i.attrib['name'] == 'SYSTEM'][0]\n",
    "\n",
    "    for final in xml_data.root.iter('structure'):\n",
    "        if(final.attrib=={'name': 'finalpos'}):\n",
    "            for i in final.iter('i'):\n",
    "                volume=float(i.text)\n",
    "            for arr in final.iter('varray'):\n",
    "                if(arr.attrib=={'name': 'basis'}):\n",
    "                    basis=[[float(a) for a in v.text.split()] for v in arr.iter('v')]\n",
    "                if(arr.attrib=={'name': 'rec_basis'}):\n",
    "                    rec_basis=[[float(a) for a in v.text.split()] for v in arr.iter('v')]\n",
    "                if(arr.attrib=={'name': 'positions'}):\n",
    "                    positions=[[float(a) for a in v.text.split()] for v in arr.iter('v')]\n",
    "    # element labels\n",
    "    types  = [int(_type.text) for _type in xml_data.root.iter('types')][0]\n",
    "    elems  = [info[0].text.strip() for info in xml_data.root.iter('rc')]\n",
    "    _inds  = np.array([int(a) for a in elems[-types:]])\n",
    "    _nums  = [k + 1 for i in _inds for k in range(i)]\n",
    "    labels = [f\"{e} {i}\" for i, e in zip(_nums,elems)]\n",
    "\n",
    "    INDS = np.cumsum([0,*_inds]).astype(int)\n",
    "    Names = list(np.unique(elems[:-types]))\n",
    "    unique_d = {e:range(INDS[i],INDS[i+1]) for i,e in enumerate(Names)}\n",
    "\n",
    "    st_dic={'SYSTEM':SYSTEM,'volume': volume,'basis': np.array(basis),'rec_basis': np.array(rec_basis),\n",
    "            'cartesian': _is_poscar_cartesian(other_path=xml_data.path),'positions': np.array(positions),'labels':labels,'unique': unique_d}\n",
    "    return serializer.PoscarData(st_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoscarData(\n",
       "    SYSTEM = unknown system\n",
       "    volume = 28.80748507\n",
       "    basis = <ndarray:shape=(3, 3)>\n",
       "    rec_basis = <ndarray:shape=(3, 3)>\n",
       "    cartesian = False\n",
       "    positions = <ndarray:shape=(3, 3)>\n",
       "    labels = ['Mg 1', 'B 1', 'B 2']\n",
       "    unique = Data(\n",
       "        B = range(0, 1)\n",
       "        Mg = range(1, 3)\n",
       "    )\n",
       "    text_plain = unknown system # Exported using pivotpy\n",
       "  3.08577910000000    \n",
       "    1.0000000000000000   -0.0000000000000000    0.0000000000000000\n",
       "   -0.5000000000000000    0.8660254164013231    0.0000000000000000\n",
       "   -0.0000000000000000   -0.0000000000000000    1.1320880551689523\n",
       "  B\tMg\n",
       "  1\t2\n",
       "Direct\n",
       "  -0.0000000000000000   0.0000000000000000  -0.0000000000000000\n",
       "   0.3333330000000000   0.6666670000000000   0.5000000000000000\n",
       "   0.6666670000000000   0.3333330000000000   0.5000000000000000\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_structure(xml_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Export for Bandstructure\n",
    "A fully comprehensive command that uses all functions and returns data for spin set 1 (set 1 and 2 if spin-polarized calculations) could be constructed for immediate usage. It is `export_vasrun()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def export_vasprun(path        = None,\n",
    "                   skipk       = None,\n",
    "                   elim        = [],\n",
    "                   kseg_inds   = [],\n",
    "                   shift_kpath = 0,\n",
    "                   try_pwsh    = True\n",
    "                   ):\n",
    "    \"\"\"\n",
    "    - Returns a full dictionary of all objects from `vasprun.xml` file. It first try to load the data exported by powershell's `Export-VR(Vasprun)`, which is very fast for large files. It is recommended to export large files in powershell first.\n",
    "    - **Parameters**\n",
    "        - path       : Path to `vasprun.xml` file. Default is `'./vasprun.xml'`.\n",
    "        - skipk      : Default is None. Automatically detects kpoints to skip.\n",
    "        - elim       : List [min,max] of energy interval. Default is [], covers all bands.\n",
    "        - kseg_inds : List of indices of kpoints where path is broken.\n",
    "        - shift_kpath: Default 0. Can be used to merge multiple calculations on single axes side by side.\n",
    "        - try_pwsh   : Default is True and tries to load data exported by `Vasp2Visual` in Powershell.\n",
    "    - **Returns**\n",
    "        - Data : Data accessible via dot notation containing nested Data objects:\n",
    "            - sys_info  : System Information\n",
    "            - dim_info  : Contains information about dimensions of returned objects.\n",
    "            - kpoints   : numpy array of kpoints with excluded IBZKPT points\n",
    "            - kpath     : 1D numpy array directly accessible for plot.\n",
    "            - bands     : Data containing bands.\n",
    "            - tdos      : Data containing total dos.\n",
    "            - pro_bands : Data containing bands projections.\n",
    "            - pro_dos   : Data containing dos projections.\n",
    "            - poscar    : Data containing basis,positions, rec_basis and volume.\n",
    "    \"\"\"\n",
    "    # Try to get files if exported data in PowerShell.\n",
    "    if try_pwsh:\n",
    "        req_files = ['Bands.txt','tDOS.txt','pDOS.txt','Projection.txt','SysInfo.py']\n",
    "        if path and os.path.isfile(path):\n",
    "            req_files = [os.path.join(\n",
    "                os.path.dirname(os.path.abspath(path)),f) for f in req_files]\n",
    "        logic = [os.path.isfile(f) for f in req_files]\n",
    "        if not False in logic:\n",
    "            print('Loading from PowerShell Exported Data...')\n",
    "            return load_export(path=(path if path else './vasprun.xml'))\n",
    "\n",
    "    # Proceed if not files from PWSH\n",
    "    if path==None:\n",
    "        path='./vasprun.xml'\n",
    "        \n",
    "    xml_data = read_asxml(path=path)\n",
    "    \n",
    "    base_dir = os.path.split(os.path.abspath(path))[0]\n",
    "    set_paths = [os.path.join(base_dir,\"_set{}.txt\".format(i)) for i in (1,2)]\n",
    "    #First exclude unnecessary kpoints. Includes only same weight points\n",
    "    if skipk!=None:\n",
    "        skipk=skipk\n",
    "    else:\n",
    "        skipk = exclude_kpts(xml_data) #that much to skip by default\n",
    "    info_dic = get_summary(xml_data) #Reads important information of system.\n",
    "    #KPOINTS\n",
    "    kpts = get_kpts(xml_data,skipk=skipk,kseg_inds=kseg_inds)\n",
    "    #EIGENVALS\n",
    "    eigenvals = get_evals(xml_data,skipk=skipk,elim=elim)\n",
    "    #TDOS\n",
    "    tot_dos = get_tdos(xml_data,spin_set=1,elim=elim)\n",
    "    #Bands and DOS Projection\n",
    "    if elim:\n",
    "        bands_range = eigenvals.indices #indices in range form.\n",
    "        grid_range=tot_dos.grid_range\n",
    "    else:\n",
    "        bands_range=None #projection function will read itself.\n",
    "        grid_range=None\n",
    "    if(info_dic.ISPIN==1):\n",
    "        pro_bands = get_bands_pro_set(xml_data=xml_data,spin_set=1,skipk=skipk,bands_range=bands_range,set_path=set_paths[0])\n",
    "        pro_dos = get_dos_pro_set(xml_data=xml_data,spin_set=1,dos_range=grid_range)\n",
    "    if(info_dic.ISPIN==2):\n",
    "        pro_1 = get_bands_pro_set(xml_data=xml_data,spin_set=1,skipk=skipk,bands_range=bands_range,set_path=set_paths[0])\n",
    "        pro_2 = get_bands_pro_set(xml_data=xml_data,spin_set=2,skipk=skipk,bands_range=bands_range,set_path=set_paths[1])\n",
    "        pros={'SpinUp': pro_1.pros,'SpinDown': pro_2.pros}#accessing spins in dictionary after .pro.\n",
    "        pro_bands={'labels':pro_1.labels,'pros': pros}\n",
    "        pdos_1 = get_dos_pro_set(xml_data=xml_data,spin_set=1,dos_range=grid_range)\n",
    "        pdos_2 = get_dos_pro_set(xml_data=xml_data,spin_set=1,dos_range=grid_range)\n",
    "        pdos={'SpinUp': pdos_1.pros,'SpinDown': pdos_2.pros}#accessing spins in dictionary after .pro.\n",
    "        pro_dos={'labels':pdos_1.labels,'pros': pdos}\n",
    "\n",
    "    #Structure\n",
    "    poscar = get_structure(xml_data = xml_data)\n",
    "    poscar = {'SYSTEM':info_dic.SYSTEM,**poscar.to_dict()}\n",
    "    #Dimensions dictionary.\n",
    "    dim_dic={'kpoints':'(NKPTS,3)','kpath':'(NKPTS,1)','bands':'⇅(NKPTS,NBANDS)','dos':'⇅(grid_size,3)','pro_dos':'⇅(NION,grid_size,en+pro_fields)','pro_bands':'⇅(NION,NKPTS,NBANDS,pro_fields)'}\n",
    "    #Writing everything to be accessible via dot notation\n",
    "    kpath=[k+shift_kpath for k in kpts.kpath]  # shift kpath for side by side calculations.\n",
    "    full_dic={'sys_info':info_dic,'dim_info':dim_dic,'kpoints':kpts.kpoints,'kpath':kpath,'bands':eigenvals,\n",
    "             'tdos':tot_dos,'pro_bands':pro_bands,'pro_dos':pro_dos,'poscar': poscar}\n",
    "    return serializer.VasprunData(full_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from PowerShell Exported Data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VasprunData(\n",
       "    sys_info = Data(\n",
       "        SYSTEM = C2\n",
       "        NION = 2\n",
       "        NELECT = 8\n",
       "        TypeION = 1\n",
       "        ElemName = ['C']\n",
       "        E_Fermi = -3.3501\n",
       "        fields = ['s', 'py', 'pz', 'px', 'dxy', 'dyz', 'dz2', 'dxz', 'x2-y2']\n",
       "        incar = Data(\n",
       "            SYSTEM = C2\n",
       "            PREC = high\n",
       "            ALGO = N\n",
       "            LSORBIT = T\n",
       "            NELMIN = 7\n",
       "            ISMEAR = 0\n",
       "            SIGMA = 0.10000000\n",
       "            LORBIT = 11\n",
       "            GGA = PS\n",
       "        )\n",
       "        ElemIndex = [0, 2]\n",
       "        ISPIN = 1\n",
       "        kpts_info = Data(\n",
       "            cartesian = False\n",
       "            header = Automatically generated using PivotPy with HSK-INDS = [0, 30, 60,-1], LABELS = ['Γ','M','K', 'Γ'], SEG-INDS = []\n",
       "        )\n",
       "    )\n",
       "    dim_info = Data(\n",
       "        kpoints = (NKPTS,3)\n",
       "        kpath = (NKPTS,1)\n",
       "        bands = ⇅(NKPTS,NBANDS)\n",
       "        dos = ⇅(grid_size,3)\n",
       "        pro_dos = ⇅(NION,grid_size,en+pro_fields)\n",
       "        pro_bands = ⇅(NION,NKPTS,NBANDS,pro_fields)\n",
       "    )\n",
       "    kpoints = <ndarray:shape=(90, 3)>\n",
       "    kpath = <list:len=90>\n",
       "    bands = Data(\n",
       "        E_Fermi = -3.3501\n",
       "        ISPIN = 1\n",
       "        NBANDS = 21\n",
       "        evals = <ndarray:shape=(90, 21)>\n",
       "        indices = range(1, 22)\n",
       "    )\n",
       "    tdos = Data(\n",
       "        E_Fermi = -3.3501\n",
       "        ISPIN = 1\n",
       "        tdos = <ndarray:shape=(301, 3)>\n",
       "    )\n",
       "    pro_bands = Data(\n",
       "        labels = ['s', 'py', 'pz', 'px', 'dxy', 'dyz', 'dz2', 'dxz', 'x2-y2']\n",
       "        pros = <ndarray:shape=(2, 90, 21, 9)>\n",
       "    )\n",
       "    pro_dos = Data(\n",
       "        labels = ['s', 'py', 'pz', 'px', 'dxy', 'dyz', 'dz2', 'dxz', 'x2-y2']\n",
       "        pros = <ndarray:shape=(2, 301, 10)>\n",
       "    )\n",
       "    poscar = Data(\n",
       "        SYSTEM = C2\n",
       "        volume = 105.49324928\n",
       "        basis = <ndarray:shape=(3, 3)>\n",
       "        rec_basis = <ndarray:shape=(3, 3)>\n",
       "        cartesian = False\n",
       "        positions = <ndarray:shape=(2, 3)>\n",
       "        labels = ['C 1', 'C 2']\n",
       "        unique = Data(\n",
       "            C = range(0, 2)\n",
       "        )\n",
       "        text_plain = C2 # Exported using pivotpy\n",
       "      2.46803100000000    \n",
       "        1.0000000000000000    0.0000000000000000    0.0000000000000000\n",
       "       -0.4999997974093519    0.8660251836382931    0.0000000000000000\n",
       "        0.0000000000000000    0.0000000000000000    8.1029342824300024\n",
       "      C\n",
       "      2\n",
       "    Direct\n",
       "       0.3333330000000000   0.6666670000000000   0.0000000000000000\n",
       "       0.6666670000000000   0.3333330000000000   0.0000000000000000\n",
       "    )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_vasprun(path='E:/Research/graphene_example/ISPIN_1/bands/vasprun.xml',elim=[-1,0],try_pwsh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _validate_evr(path_evr=None,**kwargs):\n",
    "    \"Validates data given for plotting functions. Returns a tuple of (Boolean,data).\"\n",
    "    if type(path_evr) == serializer.VasprunData:\n",
    "        return path_evr\n",
    "    \n",
    "    path_evr = path_evr or './vasprun.xml' # default path.\n",
    "    \n",
    "    if isinstance(path_evr,str):\n",
    "        if os.path.isfile(path_evr):\n",
    "            # kwargs -> skipk=skipk,elim=elim,kseg_inds=kseg_inds\n",
    "            return export_vasprun(path=path_evr,**kwargs)\n",
    "        else:\n",
    "            raise FileNotFoundError(f'File {path_evr!r} not found!')\n",
    "    # Other things are not valid.\n",
    "    raise ValueError('path_evr must be a path string or output of export_vasprun function.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Multiple Calculations\n",
    "- Sometimes one may need to compare two or more bandstructures in same figure, for that reason, it is easy to export two calculations and plot on same axis.\n",
    "- There is another situation, if you have a large supercell and split calculations into multiple ones, joining that calculations works same way, you will add the last value of first kpath into all values of next kpath and next last to next and so on, by just using `shift_kpath` in `export_vasprun` and plot each export on same axis, this will align bandstructures side by side on same axis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Exported Vasprun from PowerShell\n",
    "On Windows, it will work automatically. On Linux/Mac it may require path to powershell executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_export(path= './vasprun.xml',\n",
    "                kseg_inds =[],\n",
    "                shift_kpath = 0,\n",
    "                path_to_ps='pwsh',\n",
    "                skipk = None,\n",
    "                max_filled = 10,\n",
    "                max_empty = 10,\n",
    "                keep_files = True\n",
    "                ):\n",
    "    \"\"\"\n",
    "    - Returns a full dictionary of all objects from `vasprun.xml` file exported using powershell.\n",
    "    - **Parameters**\n",
    "        - path       : Path to `vasprun.xml` file. Default is `'./vasprun.xml'`.\n",
    "        - skipk      : Default is None. Automatically detects kpoints to skip.\n",
    "        - path_to_ps : Path to `powershell.exe`. Automatically picks on Windows and Linux if added to PATH.\n",
    "        - kseg_inds : List of indices of kpoints where path is broken.\n",
    "        - shift_kpath: Default 0. Can be used to merge multiple calculations side by side.\n",
    "        - keep_files : Could be use to clean exported text files. Default is True.\n",
    "        - max_filled : Number of filled bands below and including VBM. Default is 10.\n",
    "        - max_empty  : Number of empty bands above VBM. Default is 10.\n",
    "    - **Returns**\n",
    "        - Data : Data accessible via dot notation containing nested Data objects:\n",
    "            - sys_info  : System Information\n",
    "            - dim_info  : Contains information about dimensions of returned objects.\n",
    "            - kpoints   : numpy array of kpoints with excluded IBZKPT points\n",
    "            - kpath     : 1D numpy array directly accessible for plot.\n",
    "            - bands     : Data containing bands.\n",
    "            - tdos      : Data containing total dos.\n",
    "            - pro_bands : Data containing bands projections.\n",
    "            - pro_dos   : Data containing dos projections.\n",
    "            - poscar    : Data containing basis,positions, rec_basis and volume.\n",
    "    \"\"\"\n",
    "    that_loc, file_name = os.path.split(os.path.abspath(path)) # abspath is important to split.\n",
    "    with gu.set_dir(that_loc):\n",
    "        # Goes there and work\n",
    "        i = 0\n",
    "        required_files = ['Bands.txt','tDOS.txt','pDOS.txt','Projection.txt','SysInfo.py']\n",
    "        for _file in required_files:\n",
    "            if os.path.isfile(_file):\n",
    "               i = i + 1\n",
    "        if i < 5:\n",
    "            if skipk != None:\n",
    "                gu.ps2std(path_to_ps=path_to_ps,ps_command='Import-Module Vasp2Visual; Export-VR -InputFile {} -MaxFilled {} -MaxEmpty {} -SkipK {}'.format(path,max_filled,max_empty,skipk))\n",
    "            else:\n",
    "                gu.ps2std(path_to_ps=path_to_ps,ps_command='Import-Module Vasp2Visual; Export-VR -InputFile {} -MaxFilled {} -MaxEmpty {}'.format(path,max_filled,max_empty))\n",
    "        \n",
    "        # get info from same directory\n",
    "        iscartesian = _is_poscar_cartesian()\n",
    "        kpoints_info = get_kpoints_info()\n",
    "    \n",
    "        # Enable loading SysInfo.py file as source.\n",
    "        _vars = SourceFileLoader(\"SysInfo\", \"./SysInfo.py\").load_module()\n",
    "    \n",
    "        SYSTEM            = _vars.SYSTEM\n",
    "        NKPTS             = _vars.NKPTS\n",
    "        NBANDS            = _vars.NBANDS\n",
    "        NFILLED           = _vars.NFILLED\n",
    "        TypeION           = _vars.TypeION\n",
    "        NION              = _vars.NION\n",
    "        NELECT            = _vars.NELECT\n",
    "        nField_Projection = _vars.nField_Projection\n",
    "        E_Fermi           = _vars.E_Fermi\n",
    "        ISPIN             = _vars.ISPIN\n",
    "        ElemIndex         = _vars.ElemIndex\n",
    "        ElemName          = _vars.ElemName\n",
    "        poscar            = {'SYSTEM': SYSTEM,\n",
    "                            'volume':_vars.volume,\n",
    "                            'basis' : np.array(_vars.basis),\n",
    "                            'rec_basis': np.array(_vars.rec_basis),\n",
    "                            'cartesian':iscartesian,\n",
    "                            'positions': np.array(_vars.positions)\n",
    "                            }\n",
    "        fields            = _vars.fields\n",
    "        incar             = _vars.INCAR\n",
    "    \n",
    "        # Elements Labels\n",
    "        elem_labels = []\n",
    "        for i, name in enumerate(ElemName):\n",
    "            for ind in range(ElemIndex[i],ElemIndex[i+1]):\n",
    "                elem_labels.append(f\"{name} {str(ind - ElemIndex[i] + 1)}\")\n",
    "        poscar.update({'labels': elem_labels})\n",
    "        # Unique Elements Ranges\n",
    "        unique_d = {}\n",
    "        for i,e in enumerate(ElemName):\n",
    "            unique_d.update({e:range(ElemIndex[i],ElemIndex[i+1])})\n",
    "        poscar.update({'unique': unique_d})\n",
    "\n",
    "        # Load Data\n",
    "        bands= np.loadtxt('Bands.txt').reshape((-1,NBANDS+4)) #Must be read in 2D even if one row only.\n",
    "        start = int(open('Bands.txt').readline().split()[4][1:])\n",
    "        pro_bands= np.loadtxt('Projection.txt').reshape((-1,NBANDS*nField_Projection))\n",
    "        pro_dos = np.loadtxt('pDOS.txt')\n",
    "        dos= np.loadtxt('tDOS.txt')\n",
    "    \n",
    "        # Keep or delete only if python generates files (i < 5 case.)\n",
    "        if(keep_files==False and i==5):\n",
    "            for file in required_files:\n",
    "                os.remove(file)\n",
    "        # Returns back\n",
    "\n",
    "    # Work now!\n",
    "    sys_info = {'SYSTEM': SYSTEM,'NION': NION,'NELECT':NELECT,'TypeION': TypeION,'ElemName': ElemName, \n",
    "                'E_Fermi': E_Fermi,'fields':fields, 'incar': incar,'ElemIndex': ElemIndex,'ISPIN': ISPIN,\n",
    "                'kpts_info': kpoints_info}\n",
    "    dim_info = {'kpoints': '(NKPTS,3)','kpath': '(NKPTS,1)','bands': '⇅(NKPTS,NBANDS)','dos': '⇅(grid_size,3)',\n",
    "                'pro_dos': '⇅(NION,grid_size,en+pro_fields)','pro_bands': '⇅(NION,NKPTS,NBANDS,pro_fields)'}\n",
    "\n",
    "    bands_dic,tdos_dic,pdos_dic,pro_dic,kpath={},{},{},{},[]\n",
    "    if(ISPIN==1):\n",
    "        kpath   = bands[:,3]\n",
    "        kpoints = bands[:,:3]\n",
    "        evals   = bands[:,4:]\n",
    "        bands_dic = {'E_Fermi': E_Fermi, 'ISPIN': ISPIN, 'NBANDS': NBANDS, 'evals': evals, 'indices': range(start,start+NBANDS)}\n",
    "        tdos_dic  = {'E_Fermi': E_Fermi, 'ISPIN': ISPIN,'tdos': dos}\n",
    "        pdos      = pro_dos.reshape(NION,-1,nField_Projection+1)\n",
    "        pdos_dic  = {'labels': fields,'pros': pdos}\n",
    "        pros      = pro_bands.reshape(NION,NKPTS,NBANDS,-1)\n",
    "        pro_dic   = {'labels': fields,'pros': pros}\n",
    "    if(ISPIN==2):\n",
    "        # Bands\n",
    "        kpath   = bands[:NKPTS,3]\n",
    "        kpoints = bands[:NKPTS,:3]\n",
    "        SpinUp  = bands[:NKPTS,4:]\n",
    "        SpinDown= bands[NKPTS:,4:]\n",
    "        evals   = {'SpinUp':SpinUp,'SpinDown': SpinDown}\n",
    "        bands_dic = {'E_Fermi': E_Fermi, 'ISPIN': ISPIN, 'NBANDS': NBANDS, 'evals': evals,'indices': range(start,start+NBANDS)}\n",
    "        # tDOS\n",
    "        dlen    = int(np.shape(dos)[0]/2)\n",
    "        SpinUp  = dos[:dlen,:]\n",
    "        SpinDown= dos[dlen:,:]\n",
    "        tdos    = {'SpinUp':SpinUp,'SpinDown': SpinDown}\n",
    "        tdos_dic= {'E_Fermi': E_Fermi, 'ISPIN': ISPIN,'tdos': tdos}\n",
    "\n",
    "        # pDOS\n",
    "        plen    = int(np.shape(pro_dos)[0]/2)\n",
    "        SpinUp  = pro_dos[:plen,:].reshape(NION,-1,nField_Projection+1)\n",
    "        SpinDown= pro_dos[plen:,:].reshape(NION,-1,nField_Projection+1)\n",
    "        pdos    = {'SpinUp':SpinUp,'SpinDown': SpinDown}\n",
    "        pdos_dic= {'labels': fields,'pros': pdos}\n",
    "\n",
    "        # projections\n",
    "        pblen  = int(np.shape(pro_bands)[0]/2)\n",
    "        SpinUp  = pro_bands[:pblen,:].reshape(NION,NKPTS,NBANDS,-1)\n",
    "        SpinDown= pro_bands[pblen:,:].reshape(NION,NKPTS,NBANDS,-1)\n",
    "        pros    = {'SpinUp': SpinUp,'SpinDown': SpinDown}\n",
    "        pro_dic = {'labels': fields,'pros': pros}\n",
    "    # If broken path, then join points.\n",
    "    kpath = join_ksegments(kpath,kseg_inds)\n",
    "    kpath=[k+shift_kpath for k in kpath.copy()] # Shift kpath\n",
    "    full_dic = {'sys_info': sys_info,'dim_info': dim_info,'kpoints': kpoints,'kpath':kpath,               'bands':bands_dic,'tdos':tdos_dic,'pro_bands': pro_dic ,'pro_dos': pdos_dic,\n",
    "               'poscar':serializer.PoscarData(poscar)}\n",
    "    return serializer.VasprunData(full_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This back and forth data transport is required in [pivotpy-dash](https://github.com/massgh/pivotpy-dash) app where data is stored in browser in json format, but needs to by python objects for figures.\n",
    "\n",
    "## Write Clean data to JSON or Pickle file\n",
    "Use `pivotpy.serializer.dump` to write output of `export_vasprun` or `load_export` to pickle/json file. Pickle is useful for quick load in python while json is useful to transfer data into any language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"dump\" class=\"doc_header\"><code>dump</code><a href=\"https://github.com/massgh/pivotpy/tree/master/pivotpy/serializer.py#L227\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>dump</code>(**`dict_data`**=*`None`*, **`dump_to`**=*`'pickle'`*, **`outfile`**=*`None`*, **`indent`**=*`1`*)\n",
       "\n",
       "- Dump an [`export_vasprun`](/pivotpy/XmlElementTree.html#export_vasprun) or [`load_export`](/pivotpy/XmlElementTree.html#load_export)'s `Data` object or any dictionary to json or pickle string/file. It convert `Dict2Data` to dictionary before serializing to json/pickle, so json/pickle.loads() of converted Data would be a simple dictionary, pass that to `Dict2Data` to again make accessible via dot notation.\n",
       "- **Parameters**\n",
       "    - dict_data: Any dictionary/Dict2Data(or subclass Data) object containg numpy arrays, including [`export_vasprun`](/pivotpy/XmlElementTree.html#export_vasprun) or [`load_export`](/pivotpy/XmlElementTree.html#load_export) output.\n",
       "    - dump_to  : Defualt is `pickle` or `json`.\n",
       "    - outfile  : Defualt is None and return string. File name does not require extension.\n",
       "    - indent   : Defualt is 1. Only works for json."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"load\" class=\"doc_header\"><code>load</code><a href=\"https://github.com/massgh/pivotpy/tree/master/pivotpy/serializer.py#L257\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>load</code>(**`file_or_str`**, **`json_to`**=*`Dict2Data`*)\n",
       "\n",
       "- Loads a json/pickle dumped file or string by auto detecting it.\n",
       "- **Parameters**\n",
       "    - file_or_str : Filename of pickl/json or their string.\n",
       "    - json_to     : If loading json, convert to (Dict2Data by defualt) or subclasses and/or dict. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(serializer.dump)\n",
    "show_doc(serializer.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pivotpy as pp \n",
    "evr = pp.Vasprun('../vasprun.xml').data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(\n",
       "    SYSTEM = unknown system\n",
       "    volume = 28.80748507\n",
       "    basis = <ndarray:shape=(3, 3)>\n",
       "    rec_basis = <ndarray:shape=(3, 3)>\n",
       "    cartesian = False\n",
       "    positions = <ndarray:shape=(3, 3)>\n",
       "    labels = ['Mg 1', 'B 1', 'B 2']\n",
       "    unique = Data(\n",
       "        B = range(0, 1)\n",
       "        Mg = range(1, 3)\n",
       "    )\n",
       "    text_plain = unknown system # Exported using pivotpy\n",
       "  3.08577910000000    \n",
       "    1.0000000000000000   -0.0000000000000000    0.0000000000000000\n",
       "   -0.5000000000000000    0.8660254164013231    0.0000000000000000\n",
       "   -0.0000000000000000   -0.0000000000000000    1.1320880551689523\n",
       "  B\tMg\n",
       "  1\t2\n",
       "Direct\n",
       "  -0.0000000000000000   0.0000000000000000  -0.0000000000000000\n",
       "   0.3333330000000000   0.6666670000000000   0.5000000000000000\n",
       "   0.6666670000000000   0.3333330000000000   0.5000000000000000\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = serializer.dump(evr.poscar,dump_to='pickle')\n",
    "#print(s)\n",
    "serializer.load(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Text Files with Flexibility\n",
    "- The function `islice2array` is used to read text files which have patterns of text and numbers inline, such as EIGENVAL and PROCAR. With all the options of this function, reading and parsing of such files should take a few lines of code only. It can be used to read txt,csv tsv as well with efficent speed.\n",
    "- It reads a file without fully loading into memory and you can still access slices of data in the file. That partial data fetching from file is very handy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def islice2array(path_or_islice,dtype=float,delimiter='\\s+',\n",
    "                include=None,exclude='#',raw=False,fix_format = True,\n",
    "                start=0,nlines=None,count=-1,cols=None,new_shape=None\n",
    "                ):\n",
    "    \"\"\"\n",
    "    - Reads a sliced array from txt,csv type files and return to array. Also manages if columns lengths are not equal and return 1D array. It is faster than loading  whole file into memory. This single function could be used to parse EIGENVAL, PROCAR, DOCAR and similar files with just a combination of `exclude, include,start,stop,step` arguments.\n",
    "    - **Parameters**\n",
    "        - path_or_islice: Path/to/file or `itertools.islice(file_object)`. islice is interesting when you want to read different slices of an opened file and do not want to open it again and again. For reference on how to use it just execute `pivotpy.export_potential??` in a notebook cell or ipython terminal to see how islice is used extensively.\n",
    "        - dtype: float by default. Data type of output array, it is must have argument.\n",
    "        - start,nlines: The indices of lines to start reading from and number of lines after start respectively. Only work if `path_or_islice` is a file path. both could be None or int, while start could be a list to read slices from file provided that nlines is int. The spacing between adjacent indices in start should be equal to or greater than nlines as pointer in file do not go back on its own.  These parameters are in output of `slice_data`\n",
    "        > Note: `start` should count comments if `exclude` is None. You can use `slice_data` function to get a dictionary of `start,nlines, count, cols, new_shape` and unpack in argument instead of thinking too much.\n",
    "        - count: `np.size(output_array) = nrows x ncols`, if it is known before execution, performance is increased. This parameter is in output of `slice_data`.\n",
    "        - delimiter:  Default is `\\s+`. Could be any kind of delimiter valid in numpy and in the file.\n",
    "        - cols: List of indices of columns to pick. Useful when reading a file like PROCAR which e.g. has text and numbers inline. This parameter is in output of `slice_data`.\n",
    "        - include: Default is None and includes everything. String of patterns separated by | to keep, could be a regular expression.\n",
    "        - exclude: Default is '#' to remove comments. String of patterns separated by | to drop,could be a regular expression.\n",
    "        - raw    : Default is False, if True, returns list of raw strings. Useful to select `cols`.\n",
    "        - fix_format: Default is True, it sepearates numbers with poor formatting like 1.000-2.000 to 1.000 2.000 which is useful in PROCAR. Keep it False if want to read string literally.\n",
    "        - new_shape : Tuple of shape Default is None. Will try to reshape in this shape, if fails fallbacks to 2D or 1D. This parameter is in output of `slice_data`.\n",
    "    - **Examples**\n",
    "        > `islice2array('path/to/PROCAR',start=3,include='k-point',cols=[3,4,5])[:2]`\n",
    "        > array([[ 0.125,  0.125,  0.125],\n",
    "        >        [ 0.375,  0.125,  0.125]])\n",
    "        > `islice2array('path/to/EIGENVAL',start=7,exclude='E',cols=[1,2])[:2]`\n",
    "        > array([[-11.476913,   1.      ],\n",
    "        >        [  0.283532,   1.      ]])\n",
    "    > Note: Slicing a dimension to 100% of its data is faster than let say 80% for inner dimensions, so if you have to slice more than 50% of an inner dimension, then just load full data and slice after it.\n",
    "    \"\"\"\n",
    "    if nlines is None and isinstance(start,(list,np.ndarray)):\n",
    "        print(\"`nlines = None` with `start = array/list` is useless combination.\")\n",
    "        return np.array([]) # return empty array.\n",
    "        \n",
    "    def _fixing(_islice,include=include, exclude=exclude,fix_format=fix_format,nlines=nlines,start=start):\n",
    "        if include:\n",
    "            _islice = (l for l in _islice if re.search(include,l))\n",
    "    \n",
    "        if exclude:\n",
    "            _islice = (l for l in _islice if not re.search(exclude,l))\n",
    "    \n",
    "        # Make slices here after comment excluding.\n",
    "        if isinstance(nlines,int) and isinstance(start,(list,np.ndarray)):\n",
    "            #As islice moves the pointer as it reads, start[1:]-nlines-1\n",
    "            # This confirms spacing between two indices in start >= nlines\n",
    "            start = [start[0],*[s2-s1-nlines for s1,s2 in zip(start,start[1:])]]\n",
    "            _islice = chain(*(islice(_islice,s,s+nlines) for s in start))\n",
    "        elif isinstance(nlines,int) and isinstance(start,int):\n",
    "            _islice = islice(_islice,start,start+nlines)\n",
    "        elif nlines is None and isinstance(start,int):\n",
    "            _islice = islice(_islice,start,None)\n",
    "    \n",
    "        # Negative connected digits to avoid, especially in PROCAR\n",
    "        if fix_format:\n",
    "            _islice = (re.sub(r\"(\\d)-(\\d)\",r\"\\1 -\\2\",l) for l in _islice)\n",
    "        return _islice\n",
    "\n",
    "    def _gen(_islice,cols=cols):\n",
    "        for line in _islice:\n",
    "            line = line.strip().replace(delimiter,'  ').split()\n",
    "            if line and cols is not None: # if is must here.\n",
    "                line = [line[i] for i in cols]\n",
    "            for chars in line:\n",
    "                yield dtype(chars)\n",
    "    \n",
    "    #Process Now\n",
    "    if isinstance(path_or_islice,str) and os.path.isfile(path_or_islice):\n",
    "        with open(path_or_islice,'r') as f:\n",
    "            _islice = islice(f,0,None) # Read full, Will fix later.\n",
    "            _islice = _fixing(_islice)\n",
    "            if raw:\n",
    "                return ''.join(_islice)\n",
    "            # Must to consume islice when file is open\n",
    "            data = np.fromiter(_gen(_islice),dtype=dtype,count=count)\n",
    "    else:\n",
    "        _islice = _fixing(path_or_islice)\n",
    "        if raw:\n",
    "            return ''.join(_islice)\n",
    "        data = np.fromiter(_gen(_islice),dtype=dtype,count=count)\n",
    "    \n",
    "    if new_shape:\n",
    "        try: data = data.reshape(new_shape)\n",
    "        except: pass\n",
    "    elif cols: #Otherwise single array.\n",
    "        try: data = data.reshape((-1,len(cols)))\n",
    "        except: pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def slice_data(dim_inds,old_shape):\n",
    "    \"\"\"\n",
    "    - Returns a dictionary that can be unpacked in arguments of isclice2array function. This function works only for regular txt/csv/tsv data files which have rectangular data written.\n",
    "    - **Parameters**\n",
    "        - dim_inds : List of indices array or range to pick from each dimension. Inner dimensions are more towards right. Last itmes in dim_inds is considered to be columns. If you want to include all values in a dimension, you can put -1 in that dimension. Note that negative indexing does not work in file readig, -1 is s special case to fetch all items.\n",
    "        - old_shape: Shape of data set including the columns length in right most place.\n",
    "    - **Example**\n",
    "        - You have data as 3D arry where third dimension is along column.\n",
    "        > 0 0\n",
    "        > 0 2\n",
    "        > 1 0\n",
    "        > 1 2\n",
    "        - To pick [[0,2], [1,2]], you need to give\n",
    "        > slice_data(dim_inds = [[0,1],[1],-1], old_shape=(2,2,2))\n",
    "        > {'start': array([1, 3]), 'nlines': 1, 'count': 2}\n",
    "        - Unpack above dictionary in `islice2array` and you will get output array.\n",
    "    - Note that dimensions are packed from right to left, like 0,2 is repeating in 2nd column.\n",
    "    \"\"\"\n",
    "    # Columns are treated diffiernetly.\n",
    "    if dim_inds[-1] == -1:\n",
    "        cols = None\n",
    "    else:\n",
    "        cols = list(dim_inds[-1])\n",
    "\n",
    "    r_shape = old_shape[:-1]\n",
    "    dim_inds = dim_inds[:-1]\n",
    "    for i,ind in enumerate(dim_inds.copy()):\n",
    "        if ind == -1:\n",
    "            dim_inds[i] = range(r_shape[i])\n",
    "    nlines = 1\n",
    "    #start = [[NIONS*NBANDS*k + NIONS*b for b in _b_r] for k in range(skipk,NKPTS)] #kind of thing.\n",
    "    _prod_ = product(*dim_inds)\n",
    "    _mult_ = [np.product(r_shape[i+1:]) for i in range(len(r_shape))]\n",
    "    _out_ = np.array([np.dot(p,_mult_) for p in _prod_]).astype(int)\n",
    "    # check if innermost dimensions could be chunked.\n",
    "    step = 1\n",
    "    for i in range(-1,-len(dim_inds),-1):\n",
    "        _inds = np.array(dim_inds[i]) #innermost\n",
    "        if np.max(_inds[1:] - _inds[:-1]) == 1: # consecutive\n",
    "            step = len(_inds)\n",
    "            _out_ = _out_[::step] # Pick first indices\n",
    "            nlines = step*nlines\n",
    "            # Now check if all indices picked then make chunks in outer dimensions too.\n",
    "            if step != r_shape[i]: # Can't make chunk of outer dimension if inner is not 100% picked.\n",
    "                break # Stop more chunking\n",
    "    new_shape = [len(inds) for inds in dim_inds] #dim_inds are only in rows.\n",
    "    new_shape.append(old_shape[-1])\n",
    "    return {'start':_out_,'nlines':nlines,'count': nlines*len(_out_),'cols':cols,'new_shape':tuple(new_shape)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': array([ 49152,  98304, 147456, 196608, 245760, 294912]),\n",
       " 'nlines': 49152,\n",
       " 'count': 294912,\n",
       " 'cols': [0, 1],\n",
       " 'new_shape': (6, 768, 64, 9)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_data([list(range(1,7)),-1,-1,range(2)],old_shape=[52,768,64,9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Largs `vasprun.xml` Files\n",
    "You can split a large vasprun.xml file in a small `_vasprun.xml` file which does not contain projected data, and `_set[1,2,3,4].txt` file(s) which contain projected data of each spin set. These spin set text files can be processed by `islice2array` function efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_vasprun(path=None):\n",
    "    \"\"\"\n",
    "    - Splits a given vasprun.xml file into a smaller _vasprun.xml file plus _set[1,2,3,4].txt files which contain projected data for each spin set.\n",
    "    - **Parameters**\n",
    "        - path: path/to/vasprun.xml file.\n",
    "    - **Output**\n",
    "        - _vasprun.xml file with projected data.\n",
    "        - _set1.txt for projected data of colinear calculation.\n",
    "        - _set1.txt for spin up data and _set2.txt for spin-polarized case.\n",
    "        - _set[1,2,3,4].txt for each spin set of non-colinear calculations.\n",
    "    \"\"\"\n",
    "    if not path:\n",
    "        path = './vasprun.xml'\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(\"{!r} does not exist!\".format(path))\n",
    "    base_dir = os.path.split(os.path.abspath(path))[0]\n",
    "    out_file = os.path.join(base_dir,'_vasprun.xml')\n",
    "    out_sets = [os.path.join(base_dir,'_set{}.txt'.format(i)) for i in range(1,5)]\n",
    "    # process\n",
    "    with open(path,'r') as f:\n",
    "        lines = islice(f,None)\n",
    "        indices = [i for i,l in enumerate(lines) if re.search('projected|/eigenvalues',l)]\n",
    "        f.seek(0)\n",
    "        print(\"Writing {!r} ...\".format(out_file),end=' ')\n",
    "        with open(out_file,'w') as outf:\n",
    "            outf.write(''.join(islice(f,0,indices[1])))\n",
    "            f.seek(0)\n",
    "            outf.write(''.join(islice(f,indices[-1]+1,None)))\n",
    "            print('Done')\n",
    "            \n",
    "        f.seek(0)\n",
    "        middle = islice(f,indices[-2]+1,indices[-1]) #projected words excluded\n",
    "        spin_inds = [i for i,l in enumerate(middle) if re.search('spin',l)][1:] #first useless.\n",
    "        if len(spin_inds)>1:\n",
    "            set_length = spin_inds[1]-spin_inds[0] # Must define\n",
    "        else:\n",
    "            set_length = indices[-1]-indices[-2] #It is technically more than set length, but fine for 1 set\n",
    "        f.seek(0) # Must be at zero\n",
    "        N_sets = len(spin_inds)\n",
    "        # Let's read shape from out_file as well.\n",
    "        xml_data = read_asxml(out_file)\n",
    "        _summary = get_summary(xml_data)\n",
    "        NIONS  = _summary.NION\n",
    "        NORBS  = len(_summary.fields)\n",
    "        NBANDS = get_evals(xml_data).NBANDS\n",
    "        NKPTS  = get_kpts(xml_data).NKPTS\n",
    "        del xml_data # free meory now.\n",
    "        for i in range(N_sets): #Reads every set\n",
    "            print(\"Writing {!r} ...\".format(out_sets[i]),end=' ')\n",
    "            start = (indices[-2]+1+spin_inds[0] if i==0 else 0) # pointer is there next time.\n",
    "            stop_ = start + set_length # Should move up to set length only.\n",
    "            with open(out_sets[i],'w') as setf:\n",
    "                setf.write(\"  # Set: {} Shape: (NKPTS[NBANDS[NIONS]],NORBS) = {},{},{},{}\\n\".format(i+1,NKPTS,NBANDS,NIONS,NORBS))\n",
    "                middle = islice(f,start,stop_)\n",
    "                setf.write(''.join(l.lstrip().replace('/','').replace('<r>','') for l in middle if '</r>' in l))\n",
    "                print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data with Specific Spin Set(s)\n",
    "This is useful for spin textures, fermi surfaces etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def export_spin_data(path = None, spins = 's', skipk = None, elim = None):\n",
    "    \"\"\"\n",
    "    - Returns Data with selected spin sets. For spin polarized calculations, it returns spin up and down data.\n",
    "    - **Parameters**\n",
    "        - path   : Path to `vasprun.xml` file. Default is `'./vasprun.xml'`.\n",
    "        - skipk  : Default is None. Automatically detects kpoints to skip.\n",
    "        - elim   : List [min,max] of energy interval. Default is [], covers all bands.\n",
    "        - spins  : Spin components to include from 'sxyz', e.g. 'sx' will pick <S> and <S_x> if present.\n",
    "                   Only works if ISPIN == 1, otherwise it will be two sets for spin up and down.\n",
    "    - **Returns**\n",
    "        - Data : Data accessible via dot notation containing nested Data objects:\n",
    "            - sys_info  : System Information\n",
    "            - dim_info  : Contains information about dimensions of returned objects.\n",
    "            - kpoints   : numpy array of kpoints with excluded IBZKPT points\n",
    "            - evals     : Data containing eigenvalues as evals.<e,u,d>.\n",
    "            - spins     : Data containing bands projections as spins.<u,d,s,x,y,z>.\n",
    "            - poscar    : Data containing basis,positions, rec_basis and volume.\n",
    "    \"\"\"\n",
    "    if not isinstance(spins,str):\n",
    "        raise TypeError(f\"`spins` must be a string from 'sxyz', got {spins}!\")\n",
    "    \n",
    "    if False in [comp in 'sxyz' for comp in spins]:\n",
    "        raise ValueError(f\"`spins` must be in 'sxyz', got {spins!r}!\")\n",
    "\n",
    "    xml_data = read_asxml(path = path or './vasprun.xml')\n",
    "    \n",
    "    base_dir = os.path.split(os.path.abspath(path or './vasprun.xml'))[0]\n",
    "    set_paths = [os.path.join(base_dir,\"_set{}.txt\".format(i)) for i in (1,2,3,4)]\n",
    "    \n",
    "    skipk = skipk or exclude_kpts(xml_data=xml_data) #that much to skip by default\n",
    "    full_dic = {'sys_info':get_summary(xml_data)}\n",
    "    \n",
    "    ISPIN = full_dic['sys_info'].ISPIN\n",
    "    LSORBIT = getattr(full_dic['sys_info'].incar, 'LSORBIT', 'FALSE')\n",
    "    if 'f' in LSORBIT.lower() and ISPIN == 1:\n",
    "        for comp in spins:\n",
    "            if comp in 'xyz':\n",
    "                raise ValueError(f\"LSORBIT = {LSORBIT} does not include spin component {comp!r}!\")\n",
    "    \n",
    "    full_dic['dim_info'] = {'kpoints':'(NKPTS,3)','evals.<e,u,d>':'⇅(NKPTS,NBANDS)','spins.<u,d,s,x,y,z>':'⇅(NION,NKPTS,NBANDS,pro_fields)'}\n",
    "    full_dic['kpoints']= get_kpts(xml_data, skipk = skipk).kpoints\n",
    "    \n",
    "    bands = get_evals(xml_data, skipk = skipk,elim = elim).to_dict()\n",
    "    evals = bands['evals']\n",
    "    bands.update({'u': evals['SpinUp'], 'd': evals['SpinDown']} if ISPIN == 2 else {'e': evals})\n",
    "    del bands['evals']\n",
    "    full_dic['evals'] = bands\n",
    "    \n",
    "    bands_range = full_dic['bands'].indices if elim else None #indices in range form.\n",
    "    \n",
    "    spin_sets = {}   \n",
    "    if ISPIN == 1:\n",
    "        for n, s in enumerate('sxyz', start = 1):\n",
    "            if s in spins:\n",
    "                spin_sets[s] = get_bands_pro_set(xml_data, spin_set = n, skipk = skipk, bands_range = bands_range, set_path = set_paths[n-1]).pros\n",
    "        \n",
    "    if ISPIN == 2:\n",
    "        print(gu.color.g(f\"Found ISPIN = 2, output data got attributes spins.<u,d> instead of spins.<{','.join(spins)}>\"))\n",
    "        pro_1 = get_bands_pro_set(xml_data, spin_set = 1, skipk = skipk, bands_range = bands_range, set_path = set_paths[0])\n",
    "        pro_2 = get_bands_pro_set(xml_data, spin_set = 2, skipk = skipk, bands_range = bands_range, set_path = set_paths[1])\n",
    "        spin_sets = {'u': pro_1.pros,'d': pro_2.pros} \n",
    "        \n",
    "    full_dic['spins'] = spin_sets\n",
    "    full_dic['spins']['labels'] = full_dic['sys_info'].fields\n",
    "    full_dic['poscar'] = {'SYSTEM':full_dic['sys_info'].SYSTEM,**(get_structure(xml_data).to_dict())}\n",
    "    return serializer.SpinData(full_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<style>a{text-decoration: none !important;color:lightkblue;font-weight:bold;}\n",
       "                a:focus,a:active,a:hover{color:hotpink !important;}</style>\n",
       "> [&nbsp;`▶` Index&nbsp;](https://massgh.github.io/pivotpy/)  \n",
       "> [&nbsp;`▶` XmlElementTree●&nbsp;](https://massgh.github.io/pivotpy/XmlElementTree)  \n",
       "> [&nbsp;`▶` StaticPlots&nbsp;](https://massgh.github.io/pivotpy/StaticPlots)  \n",
       "> [&nbsp;`▶` InteractivePlots&nbsp;](https://massgh.github.io/pivotpy/InteractivePlots)  \n",
       "> [&nbsp;`▶` Utilities&nbsp;](https://massgh.github.io/pivotpy/Utilities)  \n",
       "> [&nbsp;`▶` StructureIO&nbsp;](https://massgh.github.io/pivotpy/StructureIO)  \n",
       "> [&nbsp;`▶` Widgets&nbsp;](https://massgh.github.io/pivotpy/Widgets)  \n",
       "> [&nbsp;`▶` MainAPI&nbsp;](https://massgh.github.io/pivotpy/MainAPI)  \n",
       "> [&nbsp;`▶` SpinProjectedSurfaces&nbsp;](https://massgh.github.io/pivotpy/SpinProjectedSurfaces)  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "from pivotpy.utils import nav_links \n",
    "nav_links(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
